{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises day 02 - 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_hist() -> None:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex01\n",
    "**Generate simulated values from different distributions**\n",
    "\n",
    "### a) Exponential\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Normal (Box-Mueller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_mueller_adv(num_samples : int) -> list[float]:\n",
    "    \"\"\"\n",
    "        Here we used the smart method which Bo introduced on slide 16 of the third lecture.\n",
    "        (See slide 16 of 'slide4m1.pdf')\n",
    "        NOTE: due to the nature of finding two samples at a time, the number of samples must be even.\n",
    "        If the number of samples isn't even then the function will return a list of num_samples + 1.\n",
    "        (e.g if num_samples = 5, then the function will return 6 samples)\n",
    "        \n",
    "        :param num_samples: number of samples to generate.\n",
    "        \n",
    "        :return: list of samples.\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    \n",
    "    accepted = []\n",
    "    U_1 = np.random.uniform(0.0, 1.0, num_samples)\n",
    "    V = np.random.uniform(-1.0, 1.0, (2, num_samples))\n",
    "    \n",
    "    i = 0\n",
    "    while counter < num_samples:\n",
    "        V_1 = V[0][i]\n",
    "        V_2 = V[1][i]\n",
    "        R_squared = V_1**2 + V_2**2\n",
    "        if R_squared <= 1:\n",
    "            # We accept the samples (and calculate them as per slides)\n",
    "            coeff = np.sqrt(-2 * np.log(U_1[i]))\n",
    "            R = np.sqrt(R_squared)\n",
    "            Z_1 = coeff * V_1 / R \n",
    "            Z_2 = coeff * V_2 / R\n",
    "            accepted.append(Z_1); accepted.append(Z_2)\n",
    "            counter += 2\n",
    "        if i == num_samples - 1:\n",
    "            # We generate new samples if we run out\n",
    "            i = 0\n",
    "            U_1 = np.random.uniform(0.0, 1.0, num_samples)\n",
    "            V = np.random.uniform(-1.0, 1.0, (2, num_samples))\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    return accepted\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_mueller(num_samples : int) -> list[float]:\n",
    "    \"\"\"\n",
    "        Generate num_samples samples from a standard normal distribution, by the Box-Mueller method.\n",
    "        NOTE: due to the nature of finding two samples at a time, the number of samples must be even.\n",
    "        If the number of samples isn't even then the function will return a list of num_samples + 1.\n",
    "        (e.g if num_samples = 5, then the function will return 6 samples)\n",
    "        \n",
    "        :param num_samples: The number of samples to generate.\n",
    "        \n",
    "        :return: A list of num_samples samples from a standard normal distribution.\n",
    "    \"\"\"\n",
    "    n = (num_samples // 2 + 1)\n",
    "    \n",
    "    accepted = []\n",
    "    U_1 = np.random.uniform(0.0, 1.0, n)\n",
    "    U_2 = np.random.uniform(0.0, 1.0, n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        Z_1 = np.sqrt(-2 * np.log(U_1[i])) * np.cos(2 * np.pi * U_2[i])\n",
    "        Z_2 = np.sqrt(-2 * np.log(U_1[i])) * np.sin(2 * np.pi * U_2[i])\n",
    "        \n",
    "        accepted.append(Z_1); accepted.append(Z_2)\n",
    "        \n",
    "\n",
    "    return accepted\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distribution with central limit theorem. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Pareto distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex02: Pareto distribution\n",
    "\n",
    "**For the Pareto distribution with support on $[\\beta, \\infty[$ compare mean value and variance, with analytical results, which can be calculated as E(X) = 'stuff'.\n",
    "Var(X) = 'stuff'.**\n",
    "\n",
    "We have by common decision (and by looking at slides) agreed that $k$ may assum real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Ks = 100\n",
    "NUM_BETAs = 100\n",
    "\n",
    "ks = np.linspace(2.05, 4, NUM_Ks)\n",
    "betas = np.linspace(1, 10, NUM_BETAs)\n",
    "\n",
    "pareto_means = np.zeros((NUM_Ks, NUM_BETAs))\n",
    "pareto_vars = np.zeros_like(pareto_means) # <-- damn this is new, hella cool B)\n",
    "\n",
    "pareto_E = lambda k, beta: beta * (k / (k - 1))\n",
    "pareto_Var = lambda k, beta: beta**2 * (k) / ((k-1)**2 * (k-2))\n",
    "\n",
    "\n",
    "for i, k in enumerate(ks):\n",
    "    for j, beta in enumerate(betas):\n",
    "        X = pareto_samples(k, beta, NUMS_TO_GENERATE)\n",
    "        pareto_means[i, j] = np.abs(pareto_E(k, beta) -  X.mean())\n",
    "\n",
    "        pareto_vars[i,j] = np.abs(pareto_Var(k, beta) - X.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TICKS = 6\n",
    "TICKS_ROUNDING = 2\n",
    "TICKS_DIVISOR_X = NUM_Ks // NUM_TICKS\n",
    "TICKS_DIVISOR_Y = NUM_BETAs // NUM_TICKS\n",
    "\n",
    "xs = [round(beta, TICKS_ROUNDING) for beta in betas[::TICKS_DIVISOR_X]]\n",
    "x_locs = range(NUM_Ks)[::TICKS_DIVISOR_X]\n",
    "\n",
    "ys = [round(k, TICKS_ROUNDING) for k in ks[::TICKS_DIVISOR_Y]]\n",
    "y_locs = range(NUM_BETAs)[::TICKS_DIVISOR_Y]\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Difference in means\")\n",
    "#plt.contour([ks,betas],pareto_mean)\n",
    "plt.imshow(pareto_means, cmap='hot', interpolation='nearest')\n",
    "plt.xticks(x_locs, xs)\n",
    "plt.yticks(y_locs, ys)\n",
    "plt.xlabel(\"Beta\")\n",
    "plt.ylabel(\"K\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)  \n",
    "plt.title(\"Difference in vars\")   \n",
    "plt.imshow(pareto_vars, cmap='hot', interpolation='nearest') \n",
    "plt.xticks(x_locs, xs)\n",
    "plt.yticks(y_locs, ys)\n",
    "plt.xlabel(\"Beta\")\n",
    "plt.ylabel(\"K\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sampled mean fits quite well with the theoretical expect value. There are some noticable differences, particularly for low values of $k$ and high values of $\\beta$.\n",
    "\n",
    "However, the sampled variance Vs. the theorical variance has some serious outliers. It is interesting to note that these outliers can be located in the same region as for the expected value i.e. low values of $k$ and high values of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_interval_sample(data : list[float], confidence : float = 0.95):\n",
    "    \"\"\"\n",
    "        Compute the confidence interval of the mean and varaince of the data.\n",
    "        :param data: list of data\n",
    "        :param confidence: confidence level\n",
    "        \n",
    "        :return: confidence interval of the mean of the data\n",
    "    \"\"\"\n",
    "    N = len(data)\n",
    "    alpha = 1 - confidence\n",
    "\n",
    "    data = np.array(data)\n",
    "    #conf_mean = [data.mean - confidence * data.std()/np.sqrt(N), data.mean + confidence * data.std()/np.sqrt(N)]\n",
    "    conf_mean = [data.mean() + data.std()/np.sqrt(N) * scipy.stats.t.cdf(alpha/2, N - 1), data.mean() + data.std()/np.sqrt(N) * scipy.stats.t.cdf(1-alpha/2, N - 1)]\n",
    "\n",
    "    conf_var = [((N-1)*data.var())/(scipy.stats.chi2.cdf(1- alpha/2, N - 1)),((N-1)*data.var())/(scipy.stats.chi2.cdf(alpha/2, N - 1))]\n",
    "    return conf_mean, conf_var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems normal distributed too...?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
